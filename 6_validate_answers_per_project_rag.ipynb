{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9767724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee70d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load valid projects from valid_projects.txt\n",
    "valid_projects = []\n",
    "with open('valid_project.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        project = line.strip()\n",
    "        if project:  # Ignore empty lines\n",
    "            valid_projects.append(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1643e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import types\n",
    "import time  # Added for retry logic\n",
    "\n",
    "# Nastav API klíč (ujisti se, že máš ve svém prostředí proměnnou GOOGLE_API_KEY)\n",
    "genai.configure(api_key=\"AIzaSyB3LkpREqIq8WwCFxsjXEd6-2h-Jnu1G7U\")\n",
    "\n",
    "\n",
    "def generate(input: str, questions:str, max_retries=5):\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following text about a European Project Call answer to each question. Output must be in same JSON format. Each answer must be only a number!\n",
    "\n",
    "    Text:\n",
    "    {input}\n",
    "\n",
    "    Questions:\n",
    "    {questions}\n",
    "    \"\"\"\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            # Najdi první a poslední složenou závorku a zkus to zparsovat\n",
    "            json_start = response.text.find('{')\n",
    "            json_end = response.text.rfind('}') + 1\n",
    "            json_str = response.text[json_start:json_end]\n",
    "            parsed = json.loads(json_str)\n",
    "            return parsed\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            wait_time = 2 ** retries  # Exponential backoff\n",
    "            print(f\"Retry {retries}/{max_retries} after error: {e}. Waiting {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)\n",
    "    print(\"Max retries reached. Returning None.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbecae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document chunked into 589 pieces\n"
     ]
    }
   ],
   "source": [
    "non_valid_counter = 0\n",
    "answers_score = []\n",
    "answers = []\n",
    "for call in valid_projects:\n",
    "    \n",
    "    with open(\"./test_data/\"+call+\"_questions_answers.txt\", \"r\") as file:\n",
    "        input = file.read()\n",
    "        try:\n",
    "            question_and_answers = json.loads(input)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    with open(\"./data\"+\"/\"+call+\"_combined_text.txt\", \"r\") as file:\n",
    "        input_combined = file.read()\n",
    "    \n",
    "    # Step 1: Chunk the combined text\n",
    "    chunks = chunk_text(input_combined)\n",
    "    print(f\"Document chunked into {len(chunks)} pieces\")\n",
    "    \n",
    "    # Step 2: Vectorize all chunks with E5 multilingual model\n",
    "    chunk_embeddings = model.encode(chunks)\n",
    "    print(\"All chunks vectorized\")\n",
    "    \n",
    "    # Step 3: Extract questions\n",
    "    questions = [question[\"question\"] for question in question_and_answers[\"test_questions\"]]\n",
    "    original_answers = [question[\"answer\"] for question in question_and_answers[\"test_questions\"]]\n",
    "    \n",
    "    # Step 4: Process each question to get relevant context\n",
    "    all_relevant_chunks = []\n",
    "    for question in questions:\n",
    "        # Vectorize the question\n",
    "        question_embedding = model.encode(question)\n",
    "        \n",
    "        # Find top 5 relevant chunks\n",
    "        top_chunks = find_top_k_chunks(question_embedding, chunk_embeddings, chunks, k=1)\n",
    "        \n",
    "        # Add to our collection (only the text, not the scores)\n",
    "        all_relevant_chunks.extend([chunk for chunk, _ in top_chunks])\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    unique_chunks = []\n",
    "    for chunk in all_relevant_chunks:\n",
    "        if chunk not in unique_chunks:\n",
    "            unique_chunks.append(chunk)\n",
    "    \n",
    "    # Combine all relevant chunks as context\n",
    "    rag_context = \"\\n\\n\".join(unique_chunks)\n",
    "    print(f\"RAG context created with {len(unique_chunks)} relevant chunks\")\n",
    "\n",
    "    question_and_answers_blank_answers = question_and_answers.copy()\n",
    "    for question in question_and_answers_blank_answers[\"test_questions\"]:\n",
    "        question[\"answer\"] = \"TO_BE_FILLED_BY_NUMBER\"\n",
    "\n",
    "    # Use the RAG context instead of the full combined text\n",
    "    validation_answers = generate(rag_context, json.dumps(question_and_answers_blank_answers, indent=4), max_retries=5)\n",
    "    \n",
    "    validation_answers = [question[\"answer\"] for question in validation_answers[\"test_questions\"]]\n",
    "\n",
    "    correct_answers = 0\n",
    "    for i in range(len(original_answers)):\n",
    "        if str(original_answers[i]) == str(validation_answers[i]):\n",
    "            correct_answers += 1\n",
    "\n",
    "    print(f\"Correct answers: {correct_answers}/{len(original_answers)}\")\n",
    "    answers_score.append(correct_answers/len(original_answers))\n",
    "    print(f\"Current average score: {sum(answers_score)/len(answers_score):.4f}\")\n",
    "\n",
    "\n",
    "    #Save the answers to a file folder validation_single\n",
    "    if not os.path.exists(\"validation_rag\"):\n",
    "        os.makedirs(\"validation_rag\")\n",
    "    with open(f\"validation_rag/{call}_answers.json\", \"w\") as file:\n",
    "        json.dump(validation_answers, file, indent=4) \n",
    "\n",
    "    #delay\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "print(f\"Non valid projects: {non_valid_counter}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71347c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the E5 multilingual model for embedding\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
    "\n",
    "# Function to chunk text with overlap\n",
    "def chunk_text(text, chunk_size=512, overlap=100):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        # Create a chunk that begins at the current position\n",
    "        chunk = words[i:i + chunk_size]\n",
    "        # Join the words to form a chunk\n",
    "        chunk_text = ' '.join(chunk)\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        # Stop if we've processed all the words\n",
    "        if i + chunk_size >= len(words):\n",
    "            break\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "# Function to find top k similar chunks\n",
    "def find_top_k_chunks(query_embedding, chunk_embeddings, chunks, k=10):\n",
    "    # Calculate similarity between query and all chunks\n",
    "    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
    "    \n",
    "    # Get indices of top k similarities\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # Return the top k chunks and their similarity scores\n",
    "    return [(chunks[i], similarities[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46906732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8830508474576272"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = sum(answers_score)/len(answers_score)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a03e90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
